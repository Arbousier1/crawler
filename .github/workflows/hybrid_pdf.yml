name: AI-Optimized Wiki Builder

on:
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 60 

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.24'

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install System Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable ghostscript
          pip install pymupdf

      - name: Run Go Crawler (MOMI CustomCrops)
        run: |
          # 进入 Go 项目目录
          cd go_crawler
          # 重新初始化确保依赖最新
          rm -f go.mod go.sum
          go mod init wiki-crawler
          go mod tidy
          # 运行爬虫，生成 MOMI_CustomCrops_Wiki.pdf
          go run main.go
          # 移动到根目录供后续步骤使用，并改名为 Python 脚本预期的输入名
          mv MOMI_CustomCrops_Wiki.pdf ../Wiki_Raw_Merged.pdf

      - name: Python Preliminary Clean (Optional)
        run: |
          # 检查是否有 Python 脚本，如果没有则跳过
          if [ -f python_finisher/merge.py ]; then
            python python_finisher/merge.py
          else
            echo "No python script found, skipping to Ghostscript."
          fi

      - name: Ghostscript Extreme AI Compression
        run: |
          # 极限压缩：下采样到 60dpi，删除所有非文字元数据
          # 输入文件来自上一阶段重命名的 Wiki_Raw_Merged.pdf
          gs -sDEVICE=pdfwrite -dCompatibilityLevel=1.4 \
              -dPDFSETTINGS=/screen \
              -dNOPAUSE -dQUIET -dBATCH \
              -dColorImageDownsampleType=/Bicubic -dColorImageResolution=60 \
              -dGrayImageDownsampleType=/Bicubic -dGrayImageResolution=60 \
              -dMonoImageDownsampleType=/Bicubic -dMonoImageResolution=60 \
              -sOutputFile=Wiki_AI_Final.pdf \
              Wiki_Raw_Merged.pdf

      - name: Upload Final PDF
        uses: actions/upload-artifact@v4
        with:
          name: MOMI-Wiki-Data
          path: Wiki_AI_Final.pdf
