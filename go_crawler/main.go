package main

import (
	"context"
	"fmt"
	"log"
	"net/url"
	"os"
	"path/filepath"
	"sort"
	"strings"
	"sync"
	"time"

	"github.com/chromedp/cdproto/network"
	"github.com/chromedp/cdproto/page"
	"github.com/chromedp/chromedp"
	"github.com/pdfcpu/pdfcpu/pkg/api"
	"github.com/pdfcpu/pdfcpu/pkg/pdfcpu/model"
)

const (
	BaseURL       = "https://momi.gtemc.cn/customcrops"
	OutDir        = "dist"
	FinalPDF      = "MOMI_CustomCrops_Wiki.pdf"
	MaxConcurrent = 4 // GitHub Actions å»ºè®®å€¼
)

// é’ˆå¯¹ momi.gtemc.cn çš„å‡€åŒ–è„šæœ¬
const CleanScript = `
	// 1. ç§»é™¤å¯¼èˆªæ ã€ä¾§è¾¹æ ã€å³ä¾§ç›®å½•ã€åº•éƒ¨å¯¼èˆªã€é¡µè„š
	const selectors = [
		'.navbar', 
		'.theme-doc-sidebar-container', 
		'.table-of-contents', 
		'.pagination-nav', 
		'footer',
		'.theme-doc-footer-edit-meta-row',
		'#docusaurus_skipToContent_fallback + nav'
	];
	selectors.forEach(s => document.querySelectorAll(s).forEach(e => e.remove()));

	// 2. ç§»é™¤å®½åº¦é™åˆ¶ï¼Œè®©å†…å®¹è‡ªé€‚åº” PDF
	const mainWrapper = document.querySelector('.main-wrapper');
	if(mainWrapper) mainWrapper.style.maxWidth = 'none';
	
	const docItemContainer = document.querySelector('.theme-doc-item-container');
	if(docItemContainer) {
		docItemContainer.style.maxWidth = 'none';
		docItemContainer.style.padding = '0';
	}

	// 3. å¼ºåˆ¶å±•å¼€æ‰€æœ‰ details æ ‡ç­¾
	document.querySelectorAll('details').forEach(e => e.open = true);

	// 4. è°ƒæ•´é¡µè¾¹è·
	document.body.style.margin = '20px';
	document.body.style.backgroundColor = 'white';
`

type Task struct {
	ID  int
	URL string
}

type Result struct {
	ID   int
	Path string
}

func main() {
	start := time.Now()
	os.RemoveAll(OutDir)
	os.MkdirAll(OutDir, 0755)

	opts := append(chromedp.DefaultExecAllocatorOptions[:],
		chromedp.Flag("headless", "new"),
		chromedp.Flag("disable-gpu", true),
		chromedp.Flag("no-sandbox", true),
		chromedp.Flag("disable-dev-shm-usage", true),
	)

	allocCtx, cancel := chromedp.NewExecAllocator(context.Background(), opts...)
	defer cancel()

	fmt.Println("ğŸ” æ­£åœ¨æ‰«æ momi.gtemc.cn Wiki ç›®å½•...")
	urls := scanLinks(allocCtx)
	uniqueUrls := uniqueAndSort(urls)
	fmt.Printf("âœ… å‘ç° %d ä¸ªæœ‰æ•ˆé¡µé¢ï¼Œå¼€å§‹ç”Ÿæˆ PDF...\n", len(uniqueUrls))

	taskChan := make(chan Task, len(uniqueUrls))
	resChan := make(chan Result, len(uniqueUrls))
	var wg sync.WaitGroup

	for i := 0; i < MaxConcurrent; i++ {
		wg.Add(1)
		go worker(allocCtx, taskChan, resChan, &wg)
	}

	for i, u := range uniqueUrls {
		taskChan <- Task{ID: i, URL: u}
	}
	close(taskChan)
	wg.Wait()
	close(resChan)

	var results []Result
	for r := range resChan {
		results = append(results, r)
	}
	sort.Slice(results, func(i, j int) bool { return results[i].ID < results[j].ID })

	mergePDFs(results)
	fmt.Printf("\nâœ¨ ä»»åŠ¡å®Œæˆï¼\nâ±ï¸ æ€»è€—æ—¶: %s\nğŸ“„ è¾“å‡ºæ–‡ä»¶: %s\n", time.Since(start), FinalPDF)
}

func worker(parentCtx context.Context, tasks <-chan Task, results chan<- Result, wg *sync.WaitGroup) {
	defer wg.Done()
	ctx, cancel := chromedp.NewContext(parentCtx)
	defer cancel()

	for t := range tasks {
		var buf []byte
		tCtx, tCancel := context.WithTimeout(ctx, 60*time.Second)
		
		err := chromedp.Run(tCtx,
			network.Enable(),
			network.SetBlockedURLs([]string{"*.woff*", "*.ttf", "*google-analytics*", "*analytics.js*"}),
			chromedp.Navigate(t.URL),
			chromedp.WaitReady("article"), // ç­‰å¾…æ–‡ç« ä¸»ä½“åŠ è½½
			chromedp.Sleep(2*time.Second),  // ç»™å›¾ç‰‡ç•™å‡ºåŠ è½½æ—¶é—´
			chromedp.Evaluate(CleanScript, nil),
			chromedp.ActionFunc(func(ctx context.Context) error {
				var err error
				buf, _, err = page.PrintToPDF().
					WithPrintBackground(true). // å¼€å¯èƒŒæ™¯ä»¥ä¿ç•™ä»£ç å—åº•è‰²
					WithPaperWidth(8.27).      // A4
					WithPaperHeight(11.69).
					Do(ctx)
				return err
			}),
		)
		tCancel()

		if err != nil {
			fmt.Printf("âŒ [%d] å¤±è´¥: %s\n", t.ID, t.URL)
			continue
		}

		path := filepath.Join(OutDir, fmt.Sprintf("%03d.pdf", t.ID))
		os.WriteFile(path, buf, 0644)
		results <- Result{ID: t.ID, Path: path}
		fmt.Printf("ğŸ“„ [%d/%d] å·²å®Œæˆ: %s\n", t.ID+1, MaxConcurrent, t.URL)
	}
}

func scanLinks(ctx context.Context) []string {
	ctx, cancel := chromedp.NewContext(ctx)
	defer cancel()
	
	var links []string
	toVisit := []string{BaseURL}
	visited := make(map[string]bool)
	
	for len(toVisit) > 0 {
		curr := toVisit[0]
		toVisit = toVisit[1:]
		
		// æ ¼å¼åŒ– URLï¼Œç§»é™¤ç»“å°¾æ–œæ 
		cleanCurr := strings.TrimSuffix(curr, "/")
		if visited[cleanCurr] { continue }
		visited[cleanCurr] = true
		
		// åªæœ‰åŒ…å« /docs/ çš„é¡µé¢é€šå¸¸æ‰æ˜¯å†…å®¹é¡µ
		if strings.Contains(cleanCurr, "/docs/") || cleanCurr == BaseURL {
			links = append(links, cleanCurr)
		}

		var res []string
		tCtx, tCancel := context.WithTimeout(ctx, 15*time.Second)
		chromedp.Run(tCtx, 
			chromedp.Navigate(curr),
			chromedp.WaitReady("main"),
			chromedp.Evaluate(`Array.from(document.querySelectorAll('a[href]')).map(a=>a.href)`, &res),
		)
		tCancel()

		for _, l := range res {
			u, err := url.Parse(l)
			if err != nil { continue }
			u.Fragment = "" // ç§»é™¤é”šç‚¹
			u.RawQuery = "" // ç§»é™¤å‚æ•°
			full := strings.TrimSuffix(u.String(), "/")
			
			// åªçˆ¬å–åŒç«™é“¾æ¥ï¼Œä¸”æ’é™¤æ‰ category è¿™ç§ç›®å½•ç´¢å¼•é¡µ
			if strings.HasPrefix(full, BaseURL) && 
			   !visited[full] && 
			   !strings.Contains(full, "/category/") {
				toVisit = append(toVisit, full)
			}
		}
	}
	return links
}

func mergePDFs(results []Result) {
	if len(results) == 0 { return }
	fmt.Printf("ğŸ“š æ­£åœ¨åˆå¹¶ %d ä¸ª PDF é¡µé¢...\n", len(results))
	
	var inFiles []string
	for _, r := range results {
		inFiles = append(inFiles, r.Path)
	}

	conf := model.NewDefaultConfiguration()
	// ä½¿ç”¨ Relaxed æ¨¡å¼ï¼Œå› ä¸º Docusaurus äº§ç”Ÿçš„ PDF ç»“æ„å¯èƒ½è¾ƒå¤æ‚
	conf.ValidationMode = model.ValidationRelaxed

	if err := api.MergeCreateFile(inFiles, FinalPDF, false, conf); err != nil {
		log.Fatalf("åˆå¹¶ PDF å‡ºé”™: %v", err)
	}
}

func uniqueAndSort(slice []string) []string {
	keys := make(map[string]bool)
	list := []string{}
	for _, entry := range slice {
		if _, value := keys[entry]; !value {
			keys[entry] = true
			list = append(list, entry)
		}
	}
	sort.Strings(list)
	return list
}
