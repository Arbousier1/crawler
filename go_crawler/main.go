package main

import (
	"context"
	"fmt"
	"log"
	"net/url"
	"os"
	"path/filepath"
	"sort"
	"strings"
	"sync"
	"time"

	"github.com/chromedp/cdproto/network"
	"github.com/chromedp/cdproto/page"
	"github.com/chromedp/chromedp"
	"github.com/pdfcpu/pdfcpu/pkg/api"
)

const (
	BaseURL       = "https://xiao-momi.github.io/craft-engine-wiki/"
	OutDir        = "dist"
	FinalPDF      = "Wiki_Full_Dump.pdf"
	MaxConcurrent = 4 // ä¿æŒç¨³å®š
)

// å‡€åŒ–è„šæœ¬ï¼šä¿ç•™å›¾ç‰‡ï¼Œåˆ é™¤å¯¼èˆª
const CleanScript = `
	document.querySelectorAll('nav, .sidebar, .navbar, footer, script, iframe, .theme-container > .navbar').forEach(e => e.remove());
	document.querySelectorAll('details').forEach(e => e.open = true);
	document.body.style.padding = '0px';
	document.body.style.margin = '20px';
	document.body.style.backgroundColor = 'white';
	// å°è¯•ç§»é™¤ VuePress/VitePress çš„é®ç½©
	document.querySelectorAll('.sidebar-mask').forEach(e => e.remove());
`

type Task struct {
	ID  int
	URL string
}

type Result struct {
	ID   int
	Path string
}

func main() {
	start := time.Now()
	os.RemoveAll(OutDir)
	os.MkdirAll(OutDir, 0755)

	// 1. æµè§ˆå™¨é…ç½® (å…³é”®ï¼šè®¾ç½®å¤§çª—å£)
	opts := append(chromedp.DefaultExecAllocatorOptions[:],
		chromedp.Flag("headless", "new"),
		chromedp.Flag("disable-gpu", true),
		chromedp.Flag("no-sandbox", true),
		chromedp.Flag("disable-dev-shm-usage", true),
		// ã€å…³é”®ä¿®å¤ã€‘å¼ºåˆ¶ 1920x1080ï¼Œé˜²æ­¢ä¾§è¾¹æ è¢«æŠ˜å 
		chromedp.WindowSize(1920, 1080),
	)

	allocCtx, cancel := chromedp.NewExecAllocator(context.Background(), opts...)
	defer cancel()

	// 2. æ·±åº¦é€’å½’æ‰«æ
	fmt.Println("ğŸ•·ï¸ å¯åŠ¨æ·±åº¦çˆ¬è™« (Breadth-First Search)...")
	urls := crawlAllPages(allocCtx)
	
	// å†æ¬¡å»é‡ï¼Œç¡®ä¿ä¸‡æ— ä¸€å¤±
	uniqueUrls := uniqueAndSort(urls)
	fmt.Printf("âœ… æœ€ç»ˆæ•è·: %d ä¸ªå”¯ä¸€é¡µé¢ (å‡†å¤‡æ¸²æŸ“)\n", len(uniqueUrls))

	if len(uniqueUrls) == 0 {
		log.Fatal("âŒ æœªæ‰¾åˆ°ä»»ä½•é¡µé¢ï¼Œè¯·æ£€æŸ¥ BaseURL æ˜¯å¦å¯è®¿é—®")
	}

	// 3. å¹¶å‘æ¸²æŸ“
	taskChan := make(chan Task, len(uniqueUrls))
	resChan := make(chan Result, len(uniqueUrls))
	var wg sync.WaitGroup

	for i := 0; i < MaxConcurrent; i++ {
		wg.Add(1)
		go worker(allocCtx, taskChan, resChan, &wg)
	}

	for i, u := range uniqueUrls {
		taskChan <- Task{ID: i, URL: u}
	}
	close(taskChan)
	wg.Wait()
	close(resChan)

	// 4. åˆå¹¶
	var results []Result
	for r := range resChan {
		results = append(results, r)
	}
	sort.Slice(results, func(i, j int) bool { return results[i].ID < results[j].ID })

	mergePDFs(results)
	fmt.Printf("ğŸ† å®Œæˆï¼è€—æ—¶: %s | æ–‡ä»¶: %s\n", time.Since(start), FinalPDF)
	os.RemoveAll(OutDir)
}

// crawlAllPages å®ç°äº†çœŸæ­£çš„ BFS (å¹¿åº¦ä¼˜å…ˆæœç´¢)
func crawlAllPages(rootCtx context.Context) []string {
	// åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„ browser context ç”¨äºçˆ¬å–
	ctx, cancel := chromedp.NewContext(rootCtx)
	defer cancel()

	// å¾…çˆ¬é˜Ÿåˆ—
	queue := []string{BaseURL}
	// å·²å‘ç°é›†åˆ (ç”¨äºå»é‡)
	seen := make(map[string]bool)
	seen[BaseURL] = true
	// ç»“æœåˆ—è¡¨
	var results []string

	// é™åˆ¶æœ€å¤§æ·±åº¦é˜²æ­¢æ­»å¾ªç¯ (Wikiä¸€èˆ¬ä¸è¶…è¿‡5å±‚ï¼Œä½†è¿™é‡ŒæŒ‰æ•°é‡é™åˆ¶æ›´å®‰å…¨)
	// æˆ–è€…åªè¦é˜Ÿåˆ—ä¸ç©ºå°±ä¸€ç›´çˆ¬
	for len(queue) > 0 {
		// å–å‡ºé˜Ÿé¦–
		currentURL := queue[0]
		queue = queue[1:]
		
		results = append(results, currentURL)
		fmt.Printf("ğŸ” æ‰«æä¸­ [%d Found]: %s\n", len(results), currentURL)

		// æå–è¯¥é¡µé¢ä¸Šçš„æ‰€æœ‰æ–°é“¾æ¥
		newLinks := extractLinks(ctx, currentURL)
		
		for _, link := range newLinks {
			// è§„èŒƒåŒ–é“¾æ¥ï¼šå»æ‰é”šç‚¹ï¼Œå»æ‰å°¾éƒ¨æ–œæ 
			u, err := url.Parse(link)
			if err != nil { continue }
			u.Fragment = ""
			normalizedLink := strings.TrimSuffix(u.String(), "/")

			// å¿…é¡»æ˜¯ç«™å†…é“¾æ¥ï¼Œä¸”æœªè¢«å‘ç°è¿‡
			if strings.HasPrefix(normalizedLink, BaseURL) && !seen[normalizedLink] {
				seen[normalizedLink] = true
				queue = append(queue, normalizedLink)
			}
		}
	}
	return results
}

func extractLinks(ctx context.Context, targetURL string) []string {
	// è®¾ç½®è¶…æ—¶ï¼Œé˜²æ­¢æŸä¸ªé¡µé¢å¡æ­»
	tCtx, cancel := context.WithTimeout(ctx, 15*time.Second)
	defer cancel()

	var res []string
	err := chromedp.Run(tCtx,
		chromedp.Navigate(targetURL),
		// ç­‰å¾…ä¾§è¾¹æ åŠ è½½ (VuePress å¸¸è§çš„é€‰æ‹©å™¨)
		chromedp.WaitReady("body"),
		// ç¨å¾®ç¡ä¸€ä¸‹ï¼Œç­‰ JS æ¸²æŸ“ä¾§è¾¹æ 
		chromedp.Sleep(1*time.Second),
		// æŠ“å–æ‰€æœ‰é“¾æ¥
		chromedp.Evaluate(`Array.from(document.querySelectorAll('a[href]')).map(a => a.href)`, &res),
	)
	
	if err != nil {
		// è¶…æ—¶æˆ–å‡ºé”™ä¹Ÿä¸è¦ panicï¼Œç›´æ¥è¿”å›ç©ºï¼Œç»§ç»­ä¸‹ä¸€ä¸ª
		fmt.Printf("âš ï¸ æ— æ³•æ‰«æé¡µé¢: %s (%v)\n", targetURL, err)
		return []string{}
	}
	return res
}

func worker(parentCtx context.Context, tasks <-chan Task, results chan<- Result, wg *sync.WaitGroup) {
	defer wg.Done()
	ctx, cancel := chromedp.NewContext(parentCtx)
	defer cancel()

	// æ‹¦æˆªæ— ç”¨èµ„æº (åªæ‹¦æˆªå­—ä½“å’Œè§†é¢‘ï¼Œä¿ç•™å›¾ç‰‡)
	chromedp.Run(ctx, network.Enable(), network.SetBlockedURLs([]string{
		"*.woff", "*.woff2", "*.ttf", "*.otf", "*.mp4", "*google-analytics*",
	}))

	for t := range tasks {
		var buf []byte
		tCtx, tCancel := context.WithTimeout(ctx, 45*time.Second)
		
		err := chromedp.Run(tCtx,
			chromedp.Navigate(t.URL),
			chromedp.WaitReady("body"),
			chromedp.Sleep(1500*time.Millisecond), // ç­‰å›¾ç‰‡
			chromedp.Evaluate(CleanScript, nil),
			chromedp.ActionFunc(func(ctx context.Context) error {
				buf, _, err := page.PrintToPDF().
					WithPrintBackground(false). // ä¸æ‰“å°èƒŒæ™¯
					WithPaperWidth(8.27).WithPaperHeight(11.69).
					WithMarginTop(0.3).WithMarginBottom(0.3).
					WithMarginLeft(0.3).WithMarginRight(0.3).
					Do(ctx)
				return err
			}),
		)
		tCancel()

		if err != nil {
			fmt.Printf("âš ï¸ æ¸²æŸ“å¤±è´¥: %s\n", t.URL)
			continue
		}

		path := filepath.Join(OutDir, fmt.Sprintf("%03d.pdf", t.ID))
		os.WriteFile(path, buf, 0644)
		results <- Result{ID: t.ID, Path: path}
		fmt.Printf("ğŸ“„ [%d] ä¿å­˜: %s\n", t.ID, t.URL)
	}
}

func mergePDFs(results []Result) {
	if len(results) == 0 { return }
	fmt.Println("ğŸ“š æ­£åœ¨åˆå¹¶ PDF...")
	var inFiles []string
	for _, r := range results {
		inFiles = append(inFiles, r.Path)
	}
	// ä¼ å…¥ nil ä½¿ç”¨é»˜è®¤é…ç½®
	if err := api.MergeCreateFile(inFiles, FinalPDF, false, nil); err != nil {
		log.Printf("Merge error: %v", err)
	}
}

func uniqueAndSort(slice []string) []string {
	keys := make(map[string]bool)
	list := []string{}
	for _, entry := range slice {
		if _, value := keys[entry]; !value {
			keys[entry] = true
			list = append(list, entry)
		}
	}
	sort.Strings(list)
	return list
}
